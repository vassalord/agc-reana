
RECAST_DIR = "/eos/user/s/shoienko/REANA_file"
GLOBAL_MERGED = f"{RECAST_DIR}/histograms_merged.root"
# Local stamp so Snakemake never waits for EOS directly
GLOBAL_STAMP = "eos_sync.updated"

N_FILES_MAX_PER_SAMPLE = -1
download_sleep = 0
url_prefix = "root://eospublic.cern.ch//eos/opendata"
# To run from Nebraska, you may use:
# url_prefix = "https://xrootd-local.unl.edu:1094//"

import json

def extract_samples_from_json(json_file):
    """Return list of (sample, condition) pairs and create path files for each."""
    output_files = []
    with open(json_file, "r") as fd:
        data = json.load(fd)
        for sample, conditions in data.items():
            for condition, details in conditions.items():
                sample_name = f"{sample}__{condition}"
                output_files.append(sample_name)
                # Rewrite remote paths to EOS public (unchanged, as requested)
                with open(f"sample_{sample_name}_paths.txt", "w") as path_file:
                    paths = [
                        file_info["path"].replace(
                            "https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD",
                            "root://eospublic.cern.ch//eos/opendata/cms/upload/agc/1.0.0/"
                        )
                        for file_info in details["files"]
                    ]
                    path_file.write("\n".join(paths))
    return output_files

def get_file_paths(wildcards, max=N_FILES_MAX_PER_SAMPLE):
    """Return list of at most MAX file paths for the given SAMPLE and CONDITION.
    IMPORTANT: we intentionally KEEP the original 'double slash' behavior.
    """
    with open(f"sample_{wildcards.sample}__{wildcards.condition}_paths.txt") as fd:
        filepaths = fd.read().splitlines()
    outs = [f"histograms/histograms_{wildcards.sample}__{wildcards.condition}__"+fp[38:] for fp in filepaths]
    return outs if max == -1 else outs[:max]

def get_items(json_file):
    """Return items = [(sample, condition), ...] for use in expand/comprehensions."""
    items = []
    with open(json_file, "r") as fd:
        data = json.load(fd)
        for sample, conditions in data.items():
            for condition in conditions:
                items.append((sample, condition))
    return items


_ = extract_samples_from_json("nanoaod_inputs.json")
ITEMS = get_items("nanoaod_inputs.json")
EVERYTHING_MERGED_ROOTS = [f"everything_merged_{sample}__{condition}.root" for (sample, condition) in ITEMS]

# --- DAG targets ---
rule all:
    input:
        "histograms_merged.root",
        "png_outputs/final_stack_histogram_4j1b.png",
        "png_outputs/stack_4j2b_nominal.png",
        "png_outputs/btagging_variations_4j1b_ttbar.png",
        "png_outputs/jet_energy_variations_4j2b_ttbar.png",
        "results/limits.json",
        "results/limit_summary.txt",
        GLOBAL_STAMP

# --- Processing rules ---

rule process_sample_one_file_in_sample:
    """Run the per-file analysis notebook to produce partial histograms."""
    container:
        "reanahub/reana-demo-agc-cms-ttbar-coffea:1.0.0"
    resources:
        kubernetes_memory_limit="3700Mi"
    input:
        "ttbar_analysis_reana.ipynb"
    output:
        "histograms/histograms_{sample}__{condition}__{filename}"
    params:
        sample_name = '{sample}__{condition}'
    shell:
        "/bin/bash -l && source fix-env.sh && python prepare_workspace.py sample_{params.sample_name}_{wildcards.filename} && papermill ttbar_analysis_reana.ipynb sample_{params.sample_name}_{wildcards.filename}_out.ipynb -p sample_name {params.sample_name} -p filename {url_prefix}{wildcards.filename} -k python3"

rule process_sample:
    """Merge the partial histograms per (sample, condition)."""
    container:
        "reanahub/reana-demo-agc-cms-ttbar-coffea:1.0.0"
    resources:
        kubernetes_memory_limit="1850Mi"
    input:
        "file_merging.ipynb",
        get_file_paths
    output:
        "everything_merged_{sample}__{condition}.root"
    params:
        sample_name = '{sample}__{condition}'
    shell:
        "/bin/bash -l && source fix-env.sh && papermill file_merging.ipynb merged_{params.sample_name}.ipynb -p sample_name {params.sample_name} -k python3"

rule merging_histograms:
    """Union-with-overwrite merge into a stable target: if EOS dir exists, write there; else write locally.
    Always leave a fresh local copy 'histograms_merged.root' for downstream steps.
    """
    container:
        "reanahub/reana-demo-agc-cms-ttbar-coffea:1.0.0"
    resources:
        kubernetes_memory_limit="1850Mi"
    input:
        EVERYTHING_MERGED_ROOTS,
        "final_merging.ipynb"
    output:
        "histograms_merged.root"
    params:
        recast_dir = RECAST_DIR,
        global_merged = GLOBAL_MERGED
    shell:
        r'''
        set -e
        /bin/bash -l && source fix-env.sh
        if [ -d "{params.recast_dir}" ]; then
          TGT="{params.global_merged}"
        else
          TGT="histograms_merged.root"
        fi
        # Export env var as a fallback in case papermill doesn't see the parameters cell
        export TARGET_ROOT="$TGT"
        papermill final_merging.ipynb result_notebook.ipynb -k python3 -p target_root "$TGT"
        # Ensure a local copy exists for plotting and limits
        if [ "$TGT" != "histograms_merged.root" ]; then
          cp -f "$TGT" histograms_merged.root || true
        fi
        # Avoid MissingOutputException due to FS latency
        python - <<'PY'
import os, time, sys
for _ in range(60):
    if os.path.exists("histograms_merged.root") and os.path.getsize("histograms_merged.root") > 0:
        sys.exit(0)
    time.sleep(1)
print("histograms_merged.root not found after wait", file=sys.stderr)
sys.exit(1)
PY
        '''

rule sync_to_eos:
    """Best-effort sync of the local ROOT to EOS; always writes a local stamp."""
    container:
        "reanahub/reana-demo-agc-cms-ttbar-coffea:1.0.0"
    input:
        "histograms_merged.root"
    output:
        GLOBAL_STAMP
    params:
        global_merged = GLOBAL_MERGED,
        recast_dir = RECAST_DIR
    shell:
        r'''
        set -e
        if [ -d "{params.recast_dir}" ]; then
          cp -f "{input}" "{params.global_merged}" || true
          printf "%s
" "$(date) synced to {params.global_merged}" > "{output}"
        else
          printf "%s
" "$(date) skipped EOS sync; {params.recast_dir} not available" > "{output}"
        fi
        '''

rule final_stack_histogram:
    """Make plots from the (local) merged ROOT."""
    container:
        "reanahub/reana-demo-agc-cms-ttbar-coffea:1.0.0"
    input:
        "histograms_merged.root",
        "plot_final_stack.py"
    output:
        "png_outputs/final_stack_histogram_4j1b.png",
        "png_outputs/stack_4j2b_nominal.png",
        "png_outputs/btagging_variations_4j1b_ttbar.png",
        "png_outputs/jet_energy_variations_4j2b_ttbar.png"
    shell:
        "/bin/bash -l && source fix-env.sh && python plot_final_stack.py"

rule compute_limit:
    """Run limit extraction using cabinetry on the local merged ROOT."""
    container:
        "reanahub/reana-demo-agc-cms-ttbar-coffea:1.0.0"
    resources:
        kubernetes_memory_limit="1850Mi"
    input:
        "histograms_merged.root",
        "cabinetry_config.yml",
        "cabinetry_fit_limit.py"
    output:
        "results/limits.json",
        "results/limit_summary.txt",
        "workspace.json"
    shell:
        "/bin/bash -l && source fix-env.sh && python3 cabinetry_fit_limit.py"
